-How is the communication between the host and the graphic card handled?
    It is similar to the communications within OpenGL. You create a context, a program, buffers etc. 
    The commands to be run is fed through a command queue into the GPU (a bit similar to Vulkan?).

-What function executes your kernel?
    clFinish(CommandQueue);

-How does the kernel know what element to work on?
    The kernel has a function get_global_id(0) which returns the current thread ID, which can then be compared
    to the total data lenght and executed if it checks out.

2. Reduction
-What timing did you get for your GPU reduction? Compare it to the CPU version.
    With default values: CPU 0.000004, GPU 0.000743

-Try larger data size. On what size does the GPU version get faster, or at least comparable, to the GPU?
    It never gets faster. The best we could get was CPU 0.000102, GPU 0.001042 with a data size of 16 384
    which is still an entire magnitude slower. Any larger datasets yielded us CL_OUT_OF_RESOURCES.
    This is most likely due to us using global memory in the kernel as well as relying on a single thread
    to do the final step of the computation.

-How can you optimize this further? You should know at least one way.
    You can optimize this further by using local memory on the GPU together with utilizing local thread IDs
    instead of doing everything in a global space. This would yield a lot faster results due to the number
    of global memory accesses we now are doing.

3. Bitonic
-Should each thread produce one output or two? Why?
    

-How many items can you handle in one workgroup?
    The maximum size of one workgroup is 512 elements

-What problem must be solved when you use more than one workgroup? How did you solve it?


-What time do you get? Difference to the CPU? What is the break even size? What can you expect for a parallel CPU version? (Your conclusions here may vary between the labs.)
    