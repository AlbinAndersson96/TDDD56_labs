1.1 Why does SkePU have a "fused" MapReduce?
    - Reduced memory usage? The combined MapReduce does not have to hold the entire Map result and the both input vectors (as it can map and directly reduce) 
      You also might get fewer total accesses to the vectors memory since it can do both mapping and reducing together.

    
1.2 Is there any practical reason to ever use separate Map and Reduce in sequence?
    - If you ever need to save the intermediate answer, using a separate Map + Reduce might be advantagious.


1.3 Is there a SkePU backend which is always more efficient to use, or does this depend on the problem size? Why?
    - It should depend on the problem size, expecially when talking about the GPU-backends. They should be WAY quicker on large problems while on smaller problems they would struggle due to the overhead associated with
      sending data to and from the device.

1.4 Try measuring the parallel back-ends with measureExecTime exchanged for measureExecTimeIdempotent. This measurement does a "cold run" of the lambda expression before running the proper measurement. Do you see a difference for some backends, and if so, why?
    - All backends were faster with measureExecTimeIdempotent as compared to measureExecTime. However CPU and OpenMP had pretty much the same time as before in the separate-test while showing great improvements in the MapReduce one.

2.1 Which version of the averaging filter (unified, separable) is the most efficient? Why?
    - The separable version has a simpler mathematical expression to calculate which also means that in this case, it is faster/more efficient to compute.


3.1 In data-parallel skeletons like MapOverlap, all elements areprocessed independently of each other. Is this a good fit for the medianfilter? Why/why not?
    - The calculations required for the median filter are independent from each other, which makes data-parallel skeletons like MapOverlap a good fit for this type of filter.

3.2 Describe the sequence of instructions executed in your user-function. Is it data dependent? What does this mean for e.g., automatic vectorization, or the GPU backend?
    - Our user-function first feeds the image data into an array, which is then sorted using bubble-sort and the "middle" element is chosen as return. The data-inserting step
    of this process might be data-dependent since it the next array position depends on the last position. This might (depending on how smart Skepu is) have an effect on the
    paralellization of the program. It can paralellize the independent kernels but not the code within the kernels.



Median execution timings (4 radius):
    - CPU: 0.108236 seconds
    - OpenMP: 0.0134109 seconds
    - OpenCL: 0.0011665 seconds
    - CUDA: 0.0011685 seconds

Median execution timings (16 radius):
    - CPU: 11.0785 seconds
    - OpenMP: 1.32131 seconds
    - OpenCL: 0.0004991 seconds
    - CUDA: 0.0011152 seconds